{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CORL (Clean Offline Reinforcement Learning)","text":"<p>\ud83e\uddf5 CORL is an Offline Reinforcement Learning library that provides high-quality and easy-to-follow single-file implementations  of SOTA offline reinforcement learning algorithms. Each implementation is backed by a research-friendly codebase, allowing  you to run or tune thousands of experiments. Heavily inspired by cleanrl for online RL, check them out too! The highlight features of CORL are:</p> <ul> <li>\ud83d\udcdc Single-file implementation</li> <li>\ud83d\udcc8 Benchmarked Implementation (11+ offline algorithms, 5+ offline-to-online algorithms, 30+ datasets with detailed logs )</li> <li>\ud83d\uddbc Weights and Biases integration</li> </ul> <p>You can read more about CORL design and main results in our technical paper.</p> <p>Tip</p> <pre><code>\u2b50 If you're interested in __discrete control__, make sure to check out our new library \u2014 [Katakomba](https://github.com/corl-team/katakomba). It provides both discrete control algorithms augmented with recurrence and an offline RL benchmark for the NetHack Learning environment.\n</code></pre> <p>Info</p> <pre><code>**Minari** and **Gymnasium** support: [Farama-Foundation/Minari](https://github.com/Farama-Foundation/Minari) is the\nnext generation of D4RL that will continue to be maintained and introduce new features and datasets. \nPlease see their [announcement](https://farama.org/Announcing-Minari) for further detail. \nWe are currently slowly migrating to the Minari and the progress\ncan be tracked [here](https://github.com/corl-team/CORL/issues/2). This will allow us to significantly update dependencies \nand simplify installation, and give users access to many new datasets out of the box!\n</code></pre> <p>Warning</p> <pre><code>CORL (similarily to CleanRL) is not a modular library and therefore it is not meant to be imported.\nAt the cost of duplicate code, we make all implementation details of an ORL algorithm variant easy \nto understand. You should consider using CORL if you want to 1) understand and control all implementation details \nof an algorithm or 2) rapidly prototype advanced features that other modular ORL libraries do not support.\n</code></pre>"},{"location":"#algorithms-implemented","title":"Algorithms Implemented","text":"Algorithm Variants Implemented Wandb Report Offline and Offline-to-Online \u2705 Conservative Q-Learning for Offline Reinforcement Learning (CQL) <code>offline/cql.py</code> <code>finetune/cql.py</code> docs <code>Offline</code> <code>Offline-to-online</code> \u2705 Accelerating Online Reinforcement Learning with Offline Datasets (AWAC) <code>offline/awac.py</code> <code>finetune/awac.py</code> docs <code>Offline</code> <code>Offline-to-online</code> \u2705 Offline Reinforcement Learning with Implicit Q-Learning (IQL) <code>offline/iql.py</code> <code>finetune/iql.py</code> docs <code>Offline</code> <code>Offline-to-online</code> Offline-to-Online only \u2705 Supported Policy Optimization for Offline Reinforcement Learning (SPOT) <code>finetune/spot.py</code> docs <code>Offline-to-online</code> \u2705 Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning (Cal-QL) <code>finetune/cal_ql.py</code> docs <code>Offline-to-online</code> Offline only \u2705 Behavioral Cloning (BC) <code>offline/any_percent_bc.py</code> docs <code>Offline</code> \u2705 Behavioral Cloning-10% (BC-10%) <code>offline/any_percent_bc.py</code> docs <code>Offline</code> \u2705 A Minimalist Approach to Offline Reinforcement Learning (TD3+BC) <code>offline/td3_bc.py</code> docs <code>Offline</code> \u2705 Decision Transformer: Reinforcement Learning via Sequence Modeling (DT) <code>offline/dt.py</code> docs <code>Offline</code> \u2705 Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble (SAC-N) <code>offline/sac_n.py</code> docs <code>Offline</code> \u2705 Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble (EDAC) <code>offline/edac.py</code> docs <code>Offline</code> \u2705 Revisiting the Minimalist Approach to Offline Reinforcement Learning (ReBRAC) <code>offline/rebrac.py</code> docs <code>Offline</code> \u2705 Q-Ensemble for Offline RL: Don't Scale the Ensemble, Scale the Batch Size (LB-SAC) <code>offline/lb_sac.py</code> docs <code>Offline Gym-MuJoCo</code>"},{"location":"#citing-corl","title":"Citing CORL","text":"<p>If you use CORL in your work, please use the following bibtex <pre><code>@inproceedings{\ntarasov2022corl,\n  title={CORL: Research-oriented Deep Offline Reinforcement Learning Library},\n  author={Denis Tarasov and Alexander Nikulin and Dmitry Akimov and Vladislav Kurenkov and Sergey Kolesnikov},\n  booktitle={3rd Offline RL Workshop: Offline RL as a ''Launchpad''},\n  year={2022},\n  url={https://openreview.net/forum?id=SyAS49bBcv}\n}\n</code></pre></p>"},{"location":"algorithms/awac/","title":"AWAC","text":""},{"location":"algorithms/bc/","title":"BC","text":""},{"location":"algorithms/cal-ql/","title":"Cal-QL","text":""},{"location":"algorithms/cal-ql/#overview","title":"Overview","text":"<p>Calibrated Q-Learning (Cal-QL) is an extension for offline Actor Critic algorithms that aims to improve their  offline-to-online fine-tuning performance. Offline RL algorithms that minimize out-of-distribution values for  the Q function may reduce this value too much, leading to unlearning in the early fine-tuning steps. It was originally proposed for CQL, and our implementation also builds on it. </p> <p>To solve the problem of unrealistically low Q-values, following change is made to the critic loss function (change in blue):</p> \\[ \\min _{\\phi_i} \\mathbb{E}_{\\mathbf{s}, \\mathbf{a}, \\mathbf{s}^{\\prime} \\sim \\mathcal{D}}\\left[\\left(Q_{\\phi_i}(\\mathbf{s}, \\mathbf{a})-\\left(r(\\mathbf{s}, \\mathbf{a})+\\gamma \\mathbb{E}_{\\mathbf{a}^{\\prime} \\sim \\pi_\\theta\\left(\\cdot \\mid \\mathbf{s}^{\\prime}\\right)}\\left[\\min _{j=1, 2} Q_{\\phi_j^{\\prime}}\\left(\\mathbf{s}^{\\prime}, \\mathbf{a}^{\\prime}\\right)-\\alpha \\log \\pi_\\theta\\left(\\mathbf{a}^{\\prime} \\mid \\mathbf{s}^{\\prime}\\right)\\right]\\right)\\right)^2\\right] + {\\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{D}, \\mathbf{a} \\sim \\mathcal{\\mu(a | s)}}\\left[\\color{blue}\\max(Q_{\\phi_i^{\\prime}}(s, a), V^\\nu(s))\\right] - \\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{D}, \\mathbf{a} \\sim \\mathcal{\\hat{\\pi}_\\beta(a | s)}}\\left[Q_{\\phi_i^{\\prime}}(s, a)\\right]} \\] <p>where \\(V^\\nu(s)\\) is value function approximation. Simple return-to-go calculated from the dataset is used for this purpose.</p> <p>Original paper:</p> <ul> <li>Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning</li> </ul> <p>Reference resources:</p> <ul> <li> Official codebase for Cal-QL</li> </ul> <p>Success</p> <pre><code>Cal-QL is the state-of-the-art method for challenging AntMaze domain in offline-to-online domain.\n</code></pre> <p>Warning</p> <pre><code>Cal-QL is originally based on CQL and inherits all the weaknesses which CQL has (e.g. slow training or hyperparameters sensitivity).\n</code></pre> <p>Warning</p> <pre><code>Cal-QL performs worse in offline setup than some other algorithms but finetunes much better.\n</code></pre>"},{"location":"algorithms/cal-ql/#implemented-variants","title":"Implemented Variants","text":"Variants Implemented Description <code>finetune/cal_ql.py</code> configs For continuous action spaces and offline-to-online RL."},{"location":"algorithms/cal-ql/#explanation-of-logged-metrics","title":"Explanation of logged metrics","text":"<ul> <li><code>policy_loss</code>: mean actor loss.</li> <li><code>alpha_loss</code>: mean SAC entropy loss.</li> <li><code>qf{i}_loss</code>: mean i-th critic loss.</li> <li><code>cql_q{i}_{next_actions, rand}</code>: Q mean values of i-th critic for next or random actions.</li> <li><code>d4rl_normalized_score</code>: mean evaluation normalized score. Should be between 0 and 100, where 100+ is the    performance above expert for this environment. Implemented by D4RL library [ source].</li> </ul>"},{"location":"algorithms/cal-ql/#implementation-details","title":"Implementation details","text":"<p>Also see implementation details from CQL, as Cal-QL is based on it.</p> <ol> <li>Return-to-go calculation ( algorithms/finetune/cal_ql.py#L275)</li> <li>Offline and online data constant proportion ( algorithms/finetune/cal_ql.py#L1187)</li> </ol>"},{"location":"algorithms/cal-ql/#experimental-results","title":"Experimental results","text":"<p>For detailed scores on all benchmarked datasets see benchmarks section.  Reports visually compare our reproduction results with original paper scores to make sure our implementation is working properly.</p>"},{"location":"algorithms/cal-ql/#training-options","title":"Training options","text":"<pre><code>usage: cal_ql.py [-h] [--config_path str] [--device str] [--env str] [--seed str] [--eval_seed str] [--eval_freq str] [--n_episodes str] [--offline_iterations str] [--online_iterations str] [--checkpoints_path str]\n                 [--load_model str] [--buffer_size str] [--batch_size str] [--discount str] [--alpha_multiplier str] [--use_automatic_entropy_tuning str] [--backup_entropy str] [--policy_lr str] [--qf_lr str]\n                 [--soft_target_update_rate str] [--bc_steps str] [--target_update_period str] [--cql_alpha str] [--cql_alpha_online str] [--cql_n_actions str] [--cql_importance_sample str] [--cql_lagrange str]\n                 [--cql_target_action_gap str] [--cql_temp str] [--cql_max_target_backup str] [--cql_clip_diff_min str] [--cql_clip_diff_max str] [--orthogonal_init str] [--normalize str] [--normalize_reward str]\n                 [--q_n_hidden_layers str] [--reward_scale str] [--reward_bias str] [--mixing_ratio str] [--is_sparse_reward str] [--project str] [--group str] [--name str]\n\noptions:\n  -h, --help            show this help message and exit\n  --config_path str     Path for a config file to parse with pyrallis\n\nTrainConfig:\n\n  --device str          Experiment\n  --env str             OpenAI gym environment name\n  --seed str            Sets Gym, PyTorch and Numpy seeds\n  --eval_seed str       Eval environment seed\n  --eval_freq str       How often (time steps) we evaluate\n  --n_episodes str      How many episodes run during evaluation\n  --offline_iterations str\n                        Number of offline updates\n  --online_iterations str\n                        Number of online updates\n  --checkpoints_path str\n                        Save path\n  --load_model str      Model load file name, \"\" doesn't load\n  --buffer_size str     CQL\n  --batch_size str      Batch size for all networks\n  --discount str        Discount factor\n  --alpha_multiplier str\n                        Multiplier for alpha in loss\n  --use_automatic_entropy_tuning str\n                        Tune entropy\n  --backup_entropy str  Use backup entropy\n  --policy_lr str       Policy learning rate\n  --qf_lr str           Critics learning rate\n  --soft_target_update_rate str\n                        Target network update rate\n  --bc_steps str        Number of BC steps at start\n  --target_update_period str\n                        Frequency of target nets updates\n  --cql_alpha str       CQL offline regularization parameter\n  --cql_alpha_online str\n                        CQL online regularization parameter\n  --cql_n_actions str   Number of sampled actions\n  --cql_importance_sample str\n                        Use importance sampling\n  --cql_lagrange str    Use Lagrange version of CQL\n  --cql_target_action_gap str\n                        Action gap\n  --cql_temp str        CQL temperature\n  --cql_max_target_backup str\n                        Use max target backup\n  --cql_clip_diff_min str\n                        Q-function lower loss clipping\n  --cql_clip_diff_max str\n                        Q-function upper loss clipping\n  --orthogonal_init str\n                        Orthogonal initialization\n  --normalize str       Normalize states\n  --normalize_reward str\n                        Normalize reward\n  --q_n_hidden_layers str\n                        Number of hidden layers in Q networks\n  --reward_scale str    Reward scale for normalization\n  --reward_bias str     Reward bias for normalization\n  --mixing_ratio str    Cal-QL\n  --is_sparse_reward str\n                        Use sparse reward\n  --project str         Wandb logging\n  --group str           wandb group name\n  --name str            wandb run name\n</code></pre>"},{"location":"algorithms/cql/","title":"CQL","text":""},{"location":"algorithms/cql/#overview","title":"Overview","text":"<p>Conservative Q-Learning (CQL) is among the most popular offline RL algorithms.  It is originally based on the Soft Actor Critic (SAC), but can be applied to any other method that uses a Q-function. The core idea behind CQL is to approximate Q-values for state-action pairs within the data set and to minimize this value for out-of-distribution pairs.</p> <p>This idea can be achieved with the following critic loss (change in blue):</p> \\[ \\min _{\\phi_i} \\mathbb{E}_{\\mathbf{s}, \\mathbf{a}, \\mathbf{s}^{\\prime} \\sim \\mathcal{D}} \\left[\\left(Q_{\\phi_i}(\\mathbf{s}, \\mathbf{a})-\\left(r(\\mathbf{s}, \\mathbf{a})+\\gamma \\mathbb{E}_{\\mathbf{a}^{\\prime} \\sim \\pi_\\theta\\left(\\cdot \\mid \\mathbf{s}^{\\prime}\\right)}\\left[\\min _{j=1, 2} Q_{\\phi_j^{\\prime}}\\left(\\mathbf{s}^{\\prime}, \\mathbf{a}^{\\prime}\\right)-\\alpha \\log \\pi_\\theta\\left(\\mathbf{a}^{\\prime} \\mid \\mathbf{s}^{\\prime}\\right)\\right]\\right)\\right)^2\\right] \\color{blue}{+ \\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{D}, \\mathbf{a} \\sim \\mathcal{\\mu(a | s)}}\\left[Q_{\\phi_i^{\\prime}}(s, a)\\right]} \\] <p>where \\(\\mathcal{\\mu(a | s)}\\) is sampling from the current policy with randomness.</p> <p>The authors also suggest maximizing values within the dataset for a better approximation, which should lead to the lower bound of the true values.</p> <p>The final critic loss is the following (change in blue):</p> \\[ \\min _{\\phi_i} \\mathbb{E}_{\\mathbf{s}, \\mathbf{a}, \\mathbf{s}^{\\prime} \\sim \\mathcal{D}} \\left[\\left(Q_{\\phi_i}(\\mathbf{s}, \\mathbf{a})-\\left(r(\\mathbf{s}, \\mathbf{a})+\\gamma \\mathbb{E}_{\\mathbf{a}^{\\prime} \\sim \\pi_\\theta\\left(\\cdot \\mid \\mathbf{s}^{\\prime}\\right)}\\left[\\min _{j=1, 2} Q_{\\phi_j^{\\prime}}\\left(\\mathbf{s}^{\\prime}, \\mathbf{a}^{\\prime}\\right)-\\alpha \\log \\pi_\\theta\\left(\\mathbf{a}^{\\prime} \\mid \\mathbf{s}^{\\prime}\\right)\\right]\\right)\\right)^2\\right] + \\color{blue}{\\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{D}, \\mathbf{a} \\sim \\mathcal{\\mu(a | s)}}\\left[Q_{\\phi_i^{\\prime}}(s, a)\\right] - \\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{D}, \\mathbf{a} \\sim \\mathcal{\\hat{\\pi}_\\beta(a | s)}}\\left[Q_{\\phi_i^{\\prime}}(s, a)\\right]} \\] <p>There are more details and a number of CQL variants. To learn more about them, we refer readers to the original work.</p> <p>Original paper:</p> <ul> <li>Conservative Q-Learning for Offline Reinforcement Learning</li> </ul> <p>Reference resources:</p> <ul> <li> Official codebase for CQL (does not reproduce results from the paper)</li> <li> Working unofficial implementation for CQL (Pytorch)</li> <li> Working unofficial implementation for CQL (JAX)</li> </ul> <p>Success</p> <pre><code>CQL is simple and fast in case of discrete actions space.\n</code></pre> <p>Warning</p> <pre><code>CQL has many hyperparameters, and it is very sensitive to them. For example, our implementation wasn't able to achieve reasonable results without increasing the number of critic hidden layers.\n</code></pre> <p>Warning</p> <pre><code>Due to the need in actions sampling CQL training runtime is slow comparing to other approaches. Usually it is about x4 time comparing of the backbone AC algorithm.\n</code></pre> <p>Possible extensions:</p> <ul> <li>Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning</li> </ul>"},{"location":"algorithms/cql/#implemented-variants","title":"Implemented Variants","text":"Variants Implemented Description <code>offline/cql.py</code> configs For continuous action spaces and offline RL. <code>finetune/cql.py</code> configs For continuous action spaces and offline-to-online RL."},{"location":"algorithms/cql/#explanation-of-logged-metrics","title":"Explanation of logged metrics","text":"<ul> <li><code>policy_loss</code>: mean actor loss.</li> <li><code>alpha_loss</code>: mean SAC entropy loss.</li> <li><code>qf{i}_loss</code>: mean i-th critic loss.</li> <li><code>cql_q{i}_{next_actions, rand}</code>: Q mean values of i-th critic for next or random actions.</li> <li><code>d4rl_normalized_score</code>: mean evaluation normalized score. Should be between 0 and 100, where 100+ is the    performance above expert for this environment. Implemented by D4RL library [ source].</li> </ul>"},{"location":"algorithms/cql/#implementation-details","title":"Implementation details","text":"<ol> <li>Reward scaling ( algorithms/offline/cql.py#L238)</li> <li>Increased critic size ( algorithms/offline/cql.py#L392)</li> <li>Max target backup ( algorithms/offline/cql.py#L568)</li> <li>Importance sample ( algorithms/offline/cql.py#L647)</li> <li>CQL lagrange variant ( algorithms/offline/cql.py#L681)</li> </ol>"},{"location":"algorithms/cql/#experimental-results","title":"Experimental results","text":"<p>For detailed scores on all benchmarked datasets see benchmarks section.  Reports visually compare our reproduction results with original paper scores to make sure our implementation is working properly.</p>"},{"location":"algorithms/cql/#training-options","title":"Training options","text":""},{"location":"algorithms/cql/#offlinecql","title":"<code>offline/cql</code>","text":"<pre><code>usage: cql.py [-h] [--config_path str] [--device str] [--env str] [--seed int] [--eval_freq int] [--n_episodes int]\n              [--max_timesteps int] [--checkpoints_path [str]] [--load_model str] [--buffer_size int] [--batch_size int]\n              [--discount float] [--alpha_multiplier float] [--use_automatic_entropy_tuning bool] [--backup_entropy bool]\n              [--policy_lr float] [--qf_lr float] [--soft_target_update_rate float] [--target_update_period int]\n              [--cql_n_actions int] [--cql_importance_sample bool] [--cql_lagrange bool] [--cql_target_action_gap float]\n              [--cql_temp float] [--cql_alpha float] [--cql_max_target_backup bool] [--cql_clip_diff_min float]\n              [--cql_clip_diff_max float] [--orthogonal_init bool] [--normalize bool] [--normalize_reward bool]\n              [--q_n_hidden_layers int] [--reward_scale float] [--reward_bias float] [--bc_steps int]\n              [--policy_log_std_multiplier float] [--project str] [--group str] [--name str]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config_path str     Path for a config file to parse with pyrallis (default: None)\n\nTrainConfig:\n\n  --device str\n  --env str             OpenAI gym environment name (default: halfcheetah-medium-expert-v2)\n  --seed int            Sets Gym, PyTorch and Numpy seeds (default: 0)\n  --eval_freq int       How often (time steps) we evaluate (default: 5000)\n  --n_episodes int      How many episodes run during evaluation (default: 10)\n  --max_timesteps int   Max time steps to run environment (default: 1000000)\n  --checkpoints_path [str]\n                        Save path (default: None)\n  --load_model str      Model load file name, \"\" doesn't load (default: )\n  --buffer_size int     Replay buffer size (default: 2000000)\n  --batch_size int      Batch size for all networks (default: 256)\n  --discount float      Discount factor (default: 0.99)\n  --alpha_multiplier float\n                        Multiplier for alpha in loss (default: 1.0)\n  --use_automatic_entropy_tuning bool\n                        Tune entropy (default: True)\n  --backup_entropy bool\n                        Use backup entropy (default: False)\n  --policy_lr float     Policy learning rate (default: 3e-05)\n  --qf_lr float         Critics learning rate (default: 0.0003)\n  --soft_target_update_rate float\n                        Target network update rate (default: 0.005)\n  --target_update_period int\n                        Frequency of target nets updates (default: 1)\n  --cql_n_actions int   Number of sampled actions (default: 10)\n  --cql_importance_sample bool\n                        Use importance sampling (default: True)\n  --cql_lagrange bool   Use Lagrange version of CQL (default: False)\n  --cql_target_action_gap float\n                        Action gap (default: -1.0)\n  --cql_temp float      CQL temperature (default: 1.0)\n  --cql_alpha float     Minimal Q weight (default: 10.0)\n  --cql_max_target_backup bool\n                        Use max target backup (default: False)\n  --cql_clip_diff_min float\n                        Q-function lower loss clipping (default: -inf)\n  --cql_clip_diff_max float\n                        Q-function upper loss clipping (default: inf)\n  --orthogonal_init bool\n                        Orthogonal initialization (default: True)\n  --normalize bool      Normalize states (default: True)\n  --normalize_reward bool\n                        Normalize reward (default: False)\n  --q_n_hidden_layers int\n                        Number of hidden layers in Q networks (default: 3)\n  --reward_scale float  Reward scale for normalization (default: 5.0)\n  --reward_bias float   Reward bias for normalization (default: -1.0)\n  --bc_steps int        Number of BC steps at start (default: 0)\n  --policy_log_std_multiplier float\n                        Stochastic policy std multiplier (default: 1.0)\n  --project str         wandb project name (default: CORL)\n  --group str           wandb group name (default: CQL-D4RL)\n  --name str            wandb run name (default: CQL)\n</code></pre>"},{"location":"algorithms/cql/#finetunecql","title":"<code>finetune/cql</code>","text":"<pre><code>usage: cql.py [-h] [--config_path str] [--device str] [--env str] [--seed int] [--eval_seed int] [--eval_freq int]\n              [--n_episodes int] [--offline_iterations int] [--online_iterations int] [--checkpoints_path [str]]\n              [--load_model str] [--buffer_size int] [--batch_size int] [--discount float] [--alpha_multiplier float]\n              [--use_automatic_entropy_tuning bool] [--backup_entropy bool] [--policy_lr float] [--qf_lr float]\n              [--soft_target_update_rate float] [--bc_steps int] [--target_update_period int] [--cql_alpha float]\n              [--cql_alpha_online float] [--cql_n_actions int] [--cql_importance_sample bool] [--cql_lagrange bool]\n              [--cql_target_action_gap float] [--cql_temp float] [--cql_max_target_backup bool] [--cql_clip_diff_min float]\n              [--cql_clip_diff_max float] [--orthogonal_init bool] [--normalize bool] [--normalize_reward bool]\n              [--q_n_hidden_layers int] [--reward_scale float] [--reward_bias float] [--project str] [--group str]\n              [--name str]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config_path str     Path for a config file to parse with pyrallis (default: None)\n\nTrainConfig:\n\n  --device str\n  --env str             OpenAI gym environment name (default: halfcheetah-medium-expert-v2)\n  --seed int            Sets Gym, PyTorch and Numpy seeds (default: 0)\n  --eval_seed int       Eval environment seed (default: 0)\n  --eval_freq int       How often (time steps) we evaluate (default: 5000)\n  --n_episodes int      How many episodes run during evaluation (default: 10)\n  --offline_iterations int\n                        Number of offline updates (default: 1000000)\n  --online_iterations int\n                        Number of online updates (default: 1000000)\n  --checkpoints_path [str]\n                        Save path (default: None)\n  --load_model str      Model load file name, \"\" doesn't load (default: )\n  --buffer_size int     Replay buffer size (default: 2000000)\n  --batch_size int      Batch size for all networks (default: 256)\n  --discount float      Discount factor (default: 0.99)\n  --alpha_multiplier float\n                        Multiplier for alpha in loss (default: 1.0)\n  --use_automatic_entropy_tuning bool\n                        Tune entropy (default: True)\n  --backup_entropy bool\n                        Use backup entropy (default: False)\n  --policy_lr float     Policy learning rate (default: 3e-05)\n  --qf_lr float         Critics learning rate (default: 0.0003)\n  --soft_target_update_rate float\n                        Target network update rate (default: 0.005)\n  --bc_steps int        Number of BC steps at start (default: 0)\n  --target_update_period int\n                        Frequency of target nets updates (default: 1)\n  --cql_alpha float     CQL offline regularization parameter (default: 10.0)\n  --cql_alpha_online float\n                        CQL online regularization parameter (default: 10.0)\n  --cql_n_actions int   Number of sampled actions (default: 10)\n  --cql_importance_sample bool\n                        Use importance sampling (default: True)\n  --cql_lagrange bool   Use Lagrange version of CQL (default: False)\n  --cql_target_action_gap float\n                        Action gap (default: -1.0)\n  --cql_temp float      CQL temperature (default: 1.0)\n  --cql_max_target_backup bool\n                        Use max target backup (default: False)\n  --cql_clip_diff_min float\n                        Q-function lower loss clipping (default: -inf)\n  --cql_clip_diff_max float\n                        Q-function upper loss clipping (default: inf)\n  --orthogonal_init bool\n                        Orthogonal initialization (default: True)\n  --normalize bool      Normalize states (default: True)\n  --normalize_reward bool\n                        Normalize reward (default: False)\n  --q_n_hidden_layers int\n                        Number of hidden layers in Q networks (default: 2)\n  --reward_scale float  Reward scale for normalization (default: 1.0)\n  --reward_bias float   Reward bias for normalization (default: 0.0)\n  --project str         wandb project name (default: CORL)\n  --group str           wandb group name (default: CQL-D4RL)\n  --name str            wandb run name (default: CQL)\n</code></pre>"},{"location":"algorithms/dt/","title":"DT","text":""},{"location":"algorithms/dt/#overview","title":"Overview","text":"<p>The Decision Transformer (DT) model casts offline reinforcement learning as a conditional sequence modeling problem. </p> <p>Unlike prior approaches to offline RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal  actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward-to-go), past states, and actions, Decision Transformer model can generate future actions that achieve the desired return. </p> <p>Original paper:</p> <ul> <li>Decision Transformer: Reinforcement Learning via Sequence Modeling</li> <li>Offline Reinforcement Learning as One Big Sequence Modeling Problem    (similar approach, came out at the same time)</li> </ul> <p>Reference resources:</p> <ul> <li> Official codebase for Decision Transformer</li> </ul> <p>Success</p> <pre><code>Due to the simple supervised objective and transformer architecture, Decision Transformer is simple, stable and easy to implement as it\nhas a minimum number of moving parts.\n</code></pre> <p>Warning</p> <pre><code>Despite its simplicity and stability, DT has a number of drawbacks. It does not capable of stitching suboptimal \ntrajectories (that's why poor performance on AntMaze datasets), and can also [show](https://arxiv.org/abs/2205.15967) bad performance in stochastic environments.\n</code></pre> <p>Possible extensions:</p> <ul> <li>Online Decision Transformer</li> <li>Emergent Agentic Transformer from Chain of Hindsight Experience</li> <li>Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL</li> </ul> <p>We'd be glad if someone would be interested in contributing them!</p>"},{"location":"algorithms/dt/#implemented-variants","title":"Implemented Variants","text":"Variants Implemented Description <code>offline/dt.py</code> configs For continuous action spaces and offline RL without fine-tuning support."},{"location":"algorithms/dt/#explanation-of-logged-metrics","title":"Explanation of logged metrics","text":"<ul> <li><code>eval/{target_return}_return_mean</code>: mean undiscounted evaluation return when prompted with <code>config.target_return</code> value (there might be more than one)</li> <li><code>eval/{target_return}_return_std</code>: standard deviation of the undiscounted evaluation return across <code>config.eval_episodes</code> episodes</li> <li><code>eval/{target_return}_normalized_score_mean</code>: mean normalized score when prompted with <code>config.target_return</code> value (there might be more than one).    Should be between 0 and 100, where 100+ is the performance above expert for this environment.    Implemented by D4RL library [ source].</li> <li><code>eval/{target_return}_normalized_score_std</code>: standard deviation of the normalized score return across <code>config.eval_episodes</code> episodes</li> <li><code>train_loss</code>: current training loss, Mean squared error (MSE) for continuous action spaces</li> <li><code>learning_rate</code>: current learning rate, helps monitor learning rate schedule</li> </ul>"},{"location":"algorithms/dt/#implementation-details","title":"Implementation details","text":"<ol> <li>Batch sampling weighted by trajectory length ( algorithms/offline/dt.py#L171)</li> <li>State normalization during training and inference ( algorithms/offline/dt.py#L181)</li> <li>Reward downscaling ( algorithms/offline/dt.py#L182)</li> <li>Positional embedding shared across one transition ( algorithms/offline/dt.py#L323)</li> <li>Prompting with multiple return-to-go's during evaluation, as DT can be sensitive to the prompt ( algorithms/offline/dt.py#L498)</li> </ol>"},{"location":"algorithms/dt/#experimental-results","title":"Experimental results","text":"<p>For detailed scores on all benchmarked datasets see benchmarks section.  Reports visually compare our reproduction results with original paper scores to make sure our implementation is working properly.</p>"},{"location":"algorithms/dt/#training-options","title":"Training options","text":"<pre><code>usage: dt.py [-h] [--config_path str] [--project str] [--group str] [--name str] [--embedding_dim int] [--num_layers int]\n             [--num_heads int] [--seq_len int] [--episode_len int] [--attention_dropout float] [--residual_dropout float]\n             [--embedding_dropout float] [--max_action float] [--env_name str] [--learning_rate float]\n             [--betas float float] [--weight_decay float] [--clip_grad [float]] [--batch_size int] [--update_steps int]\n             [--warmup_steps int] [--reward_scale float] [--num_workers int] [--target_returns float [float, ...]]\n             [--eval_episodes int] [--eval_every int] [--checkpoints_path [str]] [--deterministic_torch bool]\n             [--train_seed int] [--eval_seed int] [--device str]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config_path str     Path for a config file to parse with pyrallis (default: None)\n\nTrainConfig:\n\n  --project str         wandb project name (default: CORL)\n  --group str           wandb group name (default: DT-D4RL)\n  --name str            wandb run name (default: DT)\n  --embedding_dim int   transformer hidden dim (default: 128)\n  --num_layers int      depth of the transformer model (default: 3)\n  --num_heads int       number of heads in the attention (default: 1)\n  --seq_len int         maximum sequence length during training (default: 20)\n  --episode_len int     maximum rollout length, needed for the positional embeddings (default: 1000)\n  --attention_dropout float\n                        attention dropout (default: 0.1)\n  --residual_dropout float\n                        residual dropout (default: 0.1)\n  --embedding_dropout float\n                        embeddings dropout (default: 0.1)\n  --max_action float    maximum range for the symmetric actions, [-1, 1] (default: 1.0)\n  --env_name str        training dataset and evaluation environment (default: halfcheetah-medium-v2)\n  --learning_rate float\n                        AdamW optimizer learning rate (default: 0.0001)\n  --betas float float   AdamW optimizer betas (default: (0.9, 0.999))\n  --weight_decay float  AdamW weight decay (default: 0.0001)\n  --clip_grad [float]   maximum gradient norm during training, optional (default: 0.25)\n  --batch_size int      training batch size (default: 64)\n  --update_steps int    total training steps (default: 100000)\n  --warmup_steps int    warmup steps for the learning rate scheduler (default: 10000)\n  --reward_scale float  reward scaling, to reduce the magnitude (default: 0.001)\n  --num_workers int     number of workers for the pytorch dataloader (default: 4)\n  --target_returns float [float, ...]\n                        target return-to-go for the prompting durint evaluation (default: (12000.0, 6000.0))\n  --eval_episodes int   number of episodes to run during evaluation (default: 100)\n  --eval_every int      evaluation frequency, will evaluate eval_every training steps (default: 10000)\n  --checkpoints_path [str]\n                        path for checkpoints saving, optional (default: None)\n  --deterministic_torch bool\n                        configure PyTorch to use deterministic algorithms instead of nondeterministic ones (default: False)\n  --train_seed int      training random seed (default: 10)\n  --eval_seed int       evaluation random seed (default: 42)\n  --device str          training device (default: cuda)\n</code></pre>"},{"location":"algorithms/edac/","title":"EDAC","text":""},{"location":"algorithms/iql/","title":"IQL","text":""},{"location":"algorithms/lb-sac/","title":"LB-SAC","text":""},{"location":"algorithms/rebrac/","title":"ReBRAC","text":""},{"location":"algorithms/sac-n/","title":"SAC-N","text":""},{"location":"algorithms/sac-n/#overview","title":"Overview","text":"<p>SAC-N is a simple extension of well known online Soft Actor Critic (SAC) algorithm. For an overview of online SAC,  see the excellent documentation at CleanRL. SAC utilizes a conventional technique from online RL, Clipped Double Q-learning, which uses the minimum value of two parallel Q-networks  as the Bellman target. SAC-N modifies SAC by increasing the size of the Q-ensemble from \\(2\\) to \\(N\\) to prevent the overestimation. That's it!</p> <p>Critic loss (change in blue):</p> \\[ \\min _{\\phi_i} \\mathbb{E}_{\\mathbf{s}, \\mathbf{a}, \\mathbf{s}^{\\prime} \\sim \\mathcal{D}}\\left[\\left(Q_{\\phi_i}(\\mathbf{s}, \\mathbf{a})-\\left(r(\\mathbf{s}, \\mathbf{a})+\\gamma \\mathbb{E}_{\\mathbf{a}^{\\prime} \\sim \\pi_\\theta\\left(\\cdot \\mid \\mathbf{s}^{\\prime}\\right)}\\left[\\min _{\\color{blue}{j=1, \\ldots, N}} Q_{\\phi_j^{\\prime}}\\left(\\mathbf{s}^{\\prime}, \\mathbf{a}^{\\prime}\\right)-\\alpha \\log \\pi_\\theta\\left(\\mathbf{a}^{\\prime} \\mid \\mathbf{s}^{\\prime}\\right)\\right]\\right)\\right)^2\\right] \\] <p>Actor loss (change in blue):</p> \\[ \\max _\\theta \\mathbb{E}_{\\mathbf{s} \\sim \\mathcal{D}, \\mathbf{a} \\sim \\pi_\\theta(\\cdot \\mid \\mathbf{s})}\\left[\\min _{\\color{blue}{j=1, \\ldots, N}} Q_{\\phi_j}(\\mathbf{s}, \\mathbf{a})-\\alpha \\log \\pi_\\theta(\\mathbf{a} \\mid \\mathbf{s})\\right] \\] <p>Why does it work? There is a simple intuition given in the original paper. The clipped Q-learning algorithm, which chooses the  worst-case Q-value instead to compute the pessimistic estimate, can also be interpreted as utilizing the LCB of the Q-value predictions. Suppose \\(Q(s, a)\\) follows a Gaussian distribution with mean \\(m(s, a)\\) and standard deviation \\(\\sigma(s, a)\\). Also,  let \\(\\left\\{Q_j(\\mathbf{s}, \\mathbf{a})\\right\\}_{j=1}^N\\) be realizations of \\(Q(s, a)\\). Then, we can approximate the expected minimum of the realizations as</p> \\[ \\mathbb{E}\\left[\\min _{j=1, \\ldots, N} Q_j(\\mathbf{s}, \\mathbf{a})\\right] \\approx m(\\mathbf{s}, \\mathbf{a})-\\Phi^{-1}\\left(\\frac{N-\\frac{\\pi}{8}}{N-\\frac{\\pi}{4}+1}\\right) \\sigma(\\mathbf{s}, \\mathbf{a}) \\] <p>where \\(\\Phi\\) is the CDF of the standard Gaussian distribution. This relation indicates that using the clipped Q-value  is similar to penalizing the ensemble mean of the Q-values with the standard deviation scaled by a coefficient dependent on \\(N\\). For OOD actions, the standard deviation will be higher, and thus the penalty will be stronger, preventing divergence.</p> <p>Original paper:</p> <ul> <li>Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble</li> </ul> <p>Reference resources:</p> <ul> <li> Official codebase for SAC-N and EDAC</li> </ul> <p>Success</p> <pre><code>SAC-N is extremely simple extension of online SAC and works quite well out of box on majority of the benchmarks.\nUsually only one parameter needs tuning - the size of the critics ensemble. It has SOTA results on the D4RL-Mujoco domain.\n</code></pre> <p>Warning</p> <pre><code>Typically, SAC-N requires more time to converge, 3M updates instead of the usual 1M. Also, more complex tasks\nmay require a larger ensemble size, which will considerably increase training time. Finally, \nSAC-N mysteriously does not work on the AntMaze domain. If you know how to fix this, let us know, it would be awesome!\n</code></pre> <p>Possible extensions:</p> <ul> <li>Anti-Exploration by Random Network Distillation</li> <li>Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters</li> </ul> <p>We'd be glad if someone would be interested in contributing them!</p>"},{"location":"algorithms/sac-n/#implemented-variants","title":"Implemented Variants","text":"Variants Implemented Description <code>offline/sac_n.py</code> configs For continuous action spaces and offline RL without fine-tuning support."},{"location":"algorithms/sac-n/#explanation-of-logged-metrics","title":"Explanation of logged metrics","text":"<ul> <li><code>critic_loss</code>: sum of the Q-ensemble individual mean losses (for loss definition see above) </li> <li><code>actor_loss</code>: mean actor loss (for loss definition see above)</li> <li><code>alpha_loss</code>: entropy regularization coefficient loss for automatic policy entropy tuning (see CleanRL docs for more details)</li> <li><code>batch_entropy</code>: estimation of the policy distribution entropy based on the batch states</li> <li><code>alpha</code>: coefficient for entropy regularization of the policy</li> <li><code>q_policy_std</code>: standard deviation of the Q-ensemble on batch of states and policy actions</li> <li><code>q_random_std</code>: standard deviation of the Q-ensemble on batch of states and random (OOD) actions</li> <li><code>eval/reward_mean</code>: mean undiscounted evaluation return</li> <li><code>eval/reward_std</code>: standard deviation of the undiscounted evaluation return across <code>config.eval_episodes</code> episodes</li> <li><code>eval/normalized_score_mean</code>: mean evaluation normalized score. Should be between 0 and 100, where 100+ is the    performance above expert for this environment. Implemented by D4RL library [ source].</li> <li><code>eval/normalized_score_std</code>: standard deviation of the evaluation normalized score across <code>config.eval_episodes</code> episodes</li> </ul>"},{"location":"algorithms/sac-n/#implementation-details","title":"Implementation details","text":"<ol> <li>Efficient ensemble implementation with vectorized linear layers (algorithms/offline/sac_n.py#L174)</li> <li>Actor last layer initialization with small values (algorithms/offline/sac_n.py#L223)</li> <li>Critic last layer initialization with small values (but bigger than in actor) (algorithms/offline/sac_n.py#L283)</li> <li>Clipping bounds for actor <code>log_std</code> are different from original the online SAC (algorithms/offline/sac_n.py#L241)</li> </ol>"},{"location":"algorithms/sac-n/#experimental-results","title":"Experimental results","text":"<p>For detailed scores on all benchmarked datasets see benchmarks section.  Reports visually compare our reproduction results with original paper scores to make sure our implementation is working properly.</p>"},{"location":"algorithms/sac-n/#training-options","title":"Training options","text":"<pre><code>usage: sac_n.py [-h] [--config_path str] [--project str] [--group str] [--name str] [--hidden_dim int] [--num_critics int]\n                [--gamma float] [--tau float] [--actor_learning_rate float] [--critic_learning_rate float]\n                [--alpha_learning_rate float] [--max_action float] [--buffer_size int] [--env_name str] [--batch_size int]\n                [--num_epochs int] [--num_updates_on_epoch int] [--normalize_reward bool] [--eval_episodes int]\n                [--eval_every int] [--checkpoints_path [str]] [--deterministic_torch bool] [--train_seed int]\n                [--eval_seed int] [--log_every int] [--device str]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config_path str     Path for a config file to parse with pyrallis (default: None)\n\nTrainConfig:\n\n  --project str         wandb project name (default: CORL)\n  --group str           wandb group name (default: SAC-N)\n  --name str            wandb run name (default: SAC-N)\n  --hidden_dim int      actor and critic hidden dim (default: 256)\n  --num_critics int     critic ensemble size (default: 10)\n  --gamma float         discount factor (default: 0.99)\n  --tau float           coefficient for the target critic Polyak's update (default: 0.005)\n  --actor_learning_rate float\n                        actor learning rate (default: 0.0003)\n  --critic_learning_rate float\n                        critic learning rate (default: 0.0003)\n  --alpha_learning_rate float\n                        entropy coefficient learning rate for automatic tuning (default: 0.0003)\n  --max_action float    maximum range for the symmetric actions, [-1, 1] (default: 1.0)\n  --buffer_size int     maximum size of the replay buffer (default: 1000000)\n  --env_name str        training dataset and evaluation environment (default: halfcheetah-medium-v2)\n  --batch_size int      training batch size (default: 256)\n  --num_epochs int      total number of training epochs (default: 3000)\n  --num_updates_on_epoch int\n                        number of gradient updates during one epoch (default: 1000)\n  --normalize_reward bool\n                        whether to normalize reward (like in IQL) (default: False)\n  --eval_episodes int   number of episodes to run during evaluation (default: 10)\n  --eval_every int      evaluation frequency, will evaluate eval_every training steps (default: 5)\n  --checkpoints_path [str]\n                        path for checkpoints saving, optional (default: None)\n  --deterministic_torch bool\n                        configure PyTorch to use deterministic algorithms instead of nondeterministic ones (default: False)\n  --train_seed int      training random seed (default: 10)\n  --eval_seed int       evaluation random seed (default: 42)\n  --log_every int       frequency of metrics logging to the wandb (default: 100)\n  --device str          training device (default: cpu)\n</code></pre>"},{"location":"algorithms/spot/","title":"SPOT","text":""},{"location":"algorithms/td3-bc/","title":"TD3+BC","text":""},{"location":"benchmarks/offline-to-online/","title":"Offline-to-online","text":"<p>Here, we report reproduced scores after offline pretraining and online fine-tuning for all datasets and offline-to-online algorithms considered. </p> <p>Tip</p> <pre><code>If you want to re-collect our results in a more structured/nuanced manner, see [how to reproduce](repro.md) section.\n</code></pre>"},{"location":"benchmarks/offline-to-online/#scores","title":"Scores","text":""},{"location":"benchmarks/offline-to-online/#antmaze","title":"Antmaze","text":"Task-Name AWAC CQL IQL SPOT Cal-QL antmaze-umaze-v2 52.75 \u00b1 8.67 \u2192  98.75 \u00b1 1.09 94.00 \u00b1 1.58 \u2192  99.50 \u00b1 0.87 77.00 \u00b1 0.71 \u2192  96.50 \u00b1 1.12 91.00 \u00b1 2.55 \u2192  99.50 \u00b1 0.50 76.75 \u00b1 7.53 \u2192  99.75 \u00b1 0.43 antmaze-umaze-diverse-v2 56.00 \u00b1 2.74 \u2192  0.00 \u00b1 0.00 9.50 \u00b1 9.91 \u2192  99.00 \u00b1 1.22 59.50 \u00b1 9.55 \u2192  63.75 \u00b1 25.02 36.25 \u00b1 2.17 \u2192  95.00 \u00b1 3.67 32.00 \u00b1 27.79 \u2192  98.50 \u00b1 1.12 antmaze-medium-play-v2 0.00 \u00b1 0.00 \u2192  0.00 \u00b1 0.00 59.00 \u00b1 11.18 \u2192  97.75 \u00b1 1.30 71.75 \u00b1 2.95 \u2192  89.75 \u00b1 1.09 67.25 \u00b1 10.47 \u2192  97.25 \u00b1 1.30 71.75 \u00b1 3.27 \u2192  98.75 \u00b1 1.64 antmaze-medium-diverse-v2 0.00 \u00b1 0.00 \u2192  0.00 \u00b1 0.00 63.50 \u00b1 6.84 \u2192  97.25 \u00b1 1.92 64.25 \u00b1 1.92 \u2192  92.25 \u00b1 2.86 73.75 \u00b1 7.29 \u2192  94.50 \u00b1 1.66 62.00 \u00b1 4.30 \u2192  98.25 \u00b1 1.48 antmaze-large-play-v2 0.00 \u00b1 0.00 \u2192  0.00 \u00b1 0.00 28.75 \u00b1 7.76 \u2192  88.25 \u00b1 2.28 38.50 \u00b1 8.73 \u2192  64.50 \u00b1 17.04 31.50 \u00b1 12.58 \u2192  87.00 \u00b1 3.24 31.75 \u00b1 8.87 \u2192  97.25 \u00b1 1.79 antmaze-large-diverse-v2 0.00 \u00b1 0.00 \u2192  0.00 \u00b1 0.00 35.50 \u00b1 3.64 \u2192  91.75 \u00b1 3.96 26.75 \u00b1 3.77 \u2192  64.25 \u00b1 4.15 17.50 \u00b1 7.26 \u2192  81.00 \u00b1 14.14 44.00 \u00b1 8.69 \u2192  91.50 \u00b1 3.91 average 18.12 \u2192  16.46 48.38 \u2192  95.58 56.29 \u2192  78.50 52.88 \u2192  92.38 53.04 \u2192  97.33"},{"location":"benchmarks/offline-to-online/#adroit","title":"Adroit","text":"Task-Name AWAC CQL IQL SPOT Cal-QL pen-cloned-v1 88.66 \u00b1 15.10 \u2192  86.82 \u00b1 11.12 -2.76 \u00b1 0.08 \u2192  -1.28 \u00b1 2.16 84.19 \u00b1 3.96 \u2192  102.02 \u00b1 20.75 6.19 \u00b1 5.21 \u2192  43.63 \u00b1 20.09 -2.66 \u00b1 0.04 \u2192  -2.68 \u00b1 0.12 door-cloned-v1 0.93 \u00b1 1.66 \u2192  0.01 \u00b1 0.00 -0.33 \u00b1 0.01 \u2192  -0.33 \u00b1 0.01 1.19 \u00b1 0.93 \u2192  20.34 \u00b1 9.32 -0.21 \u00b1 0.14 \u2192  0.02 \u00b1 0.31 -0.33 \u00b1 0.01 \u2192  -0.33 \u00b1 0.01 hammer-cloned-v1 1.80 \u00b1 3.01 \u2192  0.24 \u00b1 0.04 0.56 \u00b1 0.55 \u2192  2.85 \u00b1 4.81 1.35 \u00b1 0.32 \u2192  57.27 \u00b1 28.49 3.97 \u00b1 6.39 \u2192  3.73 \u00b1 4.99 0.25 \u00b1 0.04 \u2192  0.17 \u00b1 0.17 relocate-cloned-v1 -0.04 \u00b1 0.04 \u2192  -0.04 \u00b1 0.01 -0.33 \u00b1 0.01 \u2192  -0.33 \u00b1 0.01 0.04 \u00b1 0.04 \u2192  0.32 \u00b1 0.38 -0.24 \u00b1 0.01 \u2192  -0.15 \u00b1 0.05 -0.31 \u00b1 0.05 \u2192  -0.31 \u00b1 0.04 average 22.84 \u2192  21.76 -0.72 \u2192  0.22 21.69 \u2192  44.99 2.43 \u2192  11.81 -0.76 \u2192  -0.79"},{"location":"benchmarks/offline-to-online/#regrets","title":"Regrets","text":""},{"location":"benchmarks/offline-to-online/#antmaze_1","title":"Antmaze","text":"Task-Name AWAC CQL IQL SPOT Cal-QL antmaze-umaze-v2 0.04 \u00b1 0.01 0.02 \u00b1 0.00 0.07 \u00b1 0.00 0.02 \u00b1 0.00 0.01 \u00b1 0.00 antmaze-umaze-diverse-v2 0.88 \u00b1 0.01 0.09 \u00b1 0.01 0.43 \u00b1 0.11 0.22 \u00b1 0.07 0.05 \u00b1 0.01 antmaze-medium-play-v2 1.00 \u00b1 0.00 0.08 \u00b1 0.01 0.09 \u00b1 0.01 0.06 \u00b1 0.00 0.04 \u00b1 0.01 antmaze-medium-diverse-v2 1.00 \u00b1 0.00 0.08 \u00b1 0.00 0.10 \u00b1 0.01 0.05 \u00b1 0.01 0.04 \u00b1 0.01 antmaze-large-play-v2 1.00 \u00b1 0.00 0.21 \u00b1 0.02 0.34 \u00b1 0.05 0.29 \u00b1 0.07 0.13 \u00b1 0.02 antmaze-large-diverse-v2 1.00 \u00b1 0.00 0.21 \u00b1 0.03 0.41 \u00b1 0.03 0.23 \u00b1 0.08 0.13 \u00b1 0.02 average 0.82 0.11 0.24 0.15 0.07"},{"location":"benchmarks/offline-to-online/#adroit_1","title":"Adroit","text":"Task-Name AWAC CQL IQL SPOT Cal-QL pen-cloned-v1 0.46 \u00b1 0.02 0.97 \u00b1 0.00 0.37 \u00b1 0.01 0.58 \u00b1 0.02 0.98 \u00b1 0.01 door-cloned-v1 1.00 \u00b1 0.00 1.00 \u00b1 0.00 0.83 \u00b1 0.03 0.99 \u00b1 0.01 1.00 \u00b1 0.00 hammer-cloned-v1 1.00 \u00b1 0.00 1.00 \u00b1 0.00 0.65 \u00b1 0.10 0.98 \u00b1 0.01 1.00 \u00b1 0.00 relocate-cloned-v1 1.00 \u00b1 0.00 1.00 \u00b1 0.00 1.00 \u00b1 0.00 1.00 \u00b1 0.00 1.00 \u00b1 0.00 average 0.86 0.99 0.71 0.89 0.99"},{"location":"benchmarks/offline-to-online/#visual-summary","title":"Visual summary","text":""},{"location":"benchmarks/offline/","title":"Offline","text":"<p>Here, we report reproduced final and best scores for all datasets and offline algorithms considered. Note that they differ by a significant   margin, and some papers may use different approaches, not making it always explicit which reporting methodology they chose. </p> <p>Tip</p> <pre><code>If you want to re-collect our results in a more structured/nuanced manner, see [how to reproduce](repro.md) section.\n</code></pre>"},{"location":"benchmarks/offline/#last-scores","title":"Last Scores","text":""},{"location":"benchmarks/offline/#gym-mujoco","title":"Gym-MuJoCo","text":"Task-Name BC 10% BC TD3+BC AWAC CQL IQL ReBRAC SAC-N EDAC DT halfcheetah-medium-v2 42.40 \u00b1 0.19 42.46 \u00b1 0.70 48.10 \u00b1 0.18 50.02 \u00b1 0.27 47.04 \u00b1 0.22 48.31 \u00b1 0.22 64.04 \u00b1 0.68 68.20 \u00b1 1.28 67.70 \u00b1 1.04 42.20 \u00b1 0.26 halfcheetah-medium-replay-v2 35.66 \u00b1 2.33 23.59 \u00b1 6.95 44.84 \u00b1 0.59 45.13 \u00b1 0.88 45.04 \u00b1 0.27 44.46 \u00b1 0.22 51.18 \u00b1 0.31 60.70 \u00b1 1.01 62.06 \u00b1 1.10 38.91 \u00b1 0.50 halfcheetah-medium-expert-v2 55.95 \u00b1 7.35 90.10 \u00b1 2.45 90.78 \u00b1 6.04 95.00 \u00b1 0.61 95.63 \u00b1 0.42 94.74 \u00b1 0.52 103.80 \u00b1 2.95 98.96 \u00b1 9.31 104.76 \u00b1 0.64 91.55 \u00b1 0.95 hopper-medium-v2 53.51 \u00b1 1.76 55.48 \u00b1 7.30 60.37 \u00b1 3.49 63.02 \u00b1 4.56 59.08 \u00b1 3.77 67.53 \u00b1 3.78 102.29 \u00b1 0.17 40.82 \u00b1 9.91 101.70 \u00b1 0.28 65.10 \u00b1 1.61 hopper-medium-replay-v2 29.81 \u00b1 2.07 70.42 \u00b1 8.66 64.42 \u00b1 21.52 98.88 \u00b1 2.07 95.11 \u00b1 5.27 97.43 \u00b1 6.39 94.98 \u00b1 6.53 100.33 \u00b1 0.78 99.66 \u00b1 0.81 81.77 \u00b1 6.87 hopper-medium-expert-v2 52.30 \u00b1 4.01 111.16 \u00b1 1.03 101.17 \u00b1 9.07 101.90 \u00b1 6.22 99.26 \u00b1 10.91 107.42 \u00b1 7.80 109.45 \u00b1 2.34 101.31 \u00b1 11.63 105.19 \u00b1 10.08 110.44 \u00b1 0.33 walker2d-medium-v2 63.23 \u00b1 16.24 67.34 \u00b1 5.17 82.71 \u00b1 4.78 68.52 \u00b1 27.19 80.75 \u00b1 3.28 80.91 \u00b1 3.17 85.82 \u00b1 0.77 87.47 \u00b1 0.66 93.36 \u00b1 1.38 67.63 \u00b1 2.54 walker2d-medium-replay-v2 21.80 \u00b1 10.15 54.35 \u00b1 6.34 85.62 \u00b1 4.01 80.62 \u00b1 3.58 73.09 \u00b1 13.22 82.15 \u00b1 3.03 84.25 \u00b1 2.25 78.99 \u00b1 0.50 87.10 \u00b1 2.78 59.86 \u00b1 2.73 walker2d-medium-expert-v2 98.96 \u00b1 15.98 108.70 \u00b1 0.25 110.03 \u00b1 0.36 111.44 \u00b1 1.62 109.56 \u00b1 0.39 111.72 \u00b1 0.86 111.86 \u00b1 0.43 114.93 \u00b1 0.41 114.75 \u00b1 0.74 107.11 \u00b1 0.96 locomotion average 50.40 69.29 76.45 79.39 78.28 81.63 89.74 83.52 92.92 73.84"},{"location":"benchmarks/offline/#maze2d","title":"Maze2d","text":"Task-Name BC 10% BC TD3+BC AWAC CQL IQL ReBRAC SAC-N EDAC DT maze2d-umaze-v1 0.36 \u00b1 8.69 12.18 \u00b1 4.29 29.41 \u00b1 12.31 65.65 \u00b1 5.34 -8.90 \u00b1 6.11 42.11 \u00b1 0.58 106.87 \u00b1 22.16 130.59 \u00b1 16.52 95.26 \u00b1 6.39 18.08 \u00b1 25.42 maze2d-medium-v1 0.79 \u00b1 3.25 14.25 \u00b1 2.33 59.45 \u00b1 36.25 84.63 \u00b1 35.54 86.11 \u00b1 9.68 34.85 \u00b1 2.72 105.11 \u00b1 31.67 88.61 \u00b1 18.72 57.04 \u00b1 3.45 31.71 \u00b1 26.33 maze2d-large-v1 2.26 \u00b1 4.39 11.32 \u00b1 5.10 97.10 \u00b1 25.41 215.50 \u00b1 3.11 23.75 \u00b1 36.70 61.72 \u00b1 3.50 78.33 \u00b1 61.77 204.76 \u00b1 1.19 95.60 \u00b1 22.92 35.66 \u00b1 28.20 maze2d average 1.13 12.58 61.99 121.92 33.65 46.23 96.77 141.32 82.64 28.48"},{"location":"benchmarks/offline/#antmaze","title":"Antmaze","text":"Task-Name BC 10% BC TD3+BC AWAC CQL IQL ReBRAC SAC-N EDAC DT antmaze-umaze-v2 55.25 \u00b1 4.15 65.75 \u00b1 5.26 70.75 \u00b1 39.18 56.75 \u00b1 9.09 92.75 \u00b1 1.92 77.00 \u00b1 5.52 97.75 \u00b1 1.48 0.00 \u00b1 0.00 0.00 \u00b1 0.00 57.00 \u00b1 9.82 antmaze-umaze-diverse-v2 47.25 \u00b1 4.09 44.00 \u00b1 1.00 44.75 \u00b1 11.61 54.75 \u00b1 8.01 37.25 \u00b1 3.70 54.25 \u00b1 5.54 83.50 \u00b1 7.02 0.00 \u00b1 0.00 0.00 \u00b1 0.00 51.75 \u00b1 0.43 antmaze-medium-play-v2 0.00 \u00b1 0.00 2.00 \u00b1 0.71 0.25 \u00b1 0.43 0.00 \u00b1 0.00 65.75 \u00b1 11.61 65.75 \u00b1 11.71 89.50 \u00b1 3.35 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 antmaze-medium-diverse-v2 0.75 \u00b1 0.83 5.75 \u00b1 9.39 0.25 \u00b1 0.43 0.00 \u00b1 0.00 67.25 \u00b1 3.56 73.75 \u00b1 5.45 83.50 \u00b1 8.20 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 antmaze-large-play-v2 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 20.75 \u00b1 7.26 42.00 \u00b1 4.53 52.25 \u00b1 29.01 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 antmaze-large-diverse-v2 0.00 \u00b1 0.00 0.75 \u00b1 0.83 0.00 \u00b1 0.00 0.00 \u00b1 0.00 20.50 \u00b1 13.24 30.25 \u00b1 3.63 64.00 \u00b1 5.43 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 antmaze average 17.21 19.71 19.33 18.58 50.71 57.17 78.42 0.00 0.00 18.12"},{"location":"benchmarks/offline/#adroit","title":"Adroit","text":"Task-Name BC 10% BC TD3+BC AWAC CQL IQL ReBRAC SAC-N EDAC DT pen-human-v1 71.03 \u00b1 6.26 26.99 \u00b1 9.60 -3.88 \u00b1 0.21 76.65 \u00b1 11.71 13.71 \u00b1 16.98 78.49 \u00b1 8.21 103.16 \u00b1 8.49 6.86 \u00b1 5.93 5.07 \u00b1 6.16 67.68 \u00b1 5.48 pen-cloned-v1 51.92 \u00b1 15.15 46.67 \u00b1 14.25 5.13 \u00b1 5.28 85.72 \u00b1 16.92 1.04 \u00b1 6.62 83.42 \u00b1 8.19 102.79 \u00b1 7.84 31.35 \u00b1 2.14 12.02 \u00b1 1.75 64.43 \u00b1 1.43 pen-expert-v1 109.65 \u00b1 7.28 114.96 \u00b1 2.96 122.53 \u00b1 21.27 159.91 \u00b1 1.87 -1.41 \u00b1 2.34 128.05 \u00b1 9.21 152.16 \u00b1 6.33 87.11 \u00b1 48.95 -1.55 \u00b1 0.81 116.38 \u00b1 1.27 door-human-v1 2.34 \u00b1 4.00 -0.13 \u00b1 0.07 -0.33 \u00b1 0.01 2.39 \u00b1 2.26 5.53 \u00b1 1.31 3.26 \u00b1 1.83 -0.10 \u00b1 0.01 -0.38 \u00b1 0.00 -0.12 \u00b1 0.13 4.44 \u00b1 0.87 door-cloned-v1 -0.09 \u00b1 0.03 0.29 \u00b1 0.59 -0.34 \u00b1 0.01 -0.01 \u00b1 0.01 -0.33 \u00b1 0.01 3.07 \u00b1 1.75 0.06 \u00b1 0.05 -0.33 \u00b1 0.00 2.66 \u00b1 2.31 7.64 \u00b1 3.26 door-expert-v1 105.35 \u00b1 0.09 104.04 \u00b1 1.46 -0.33 \u00b1 0.01 104.57 \u00b1 0.31 -0.32 \u00b1 0.02 106.65 \u00b1 0.25 106.37 \u00b1 0.29 -0.33 \u00b1 0.00 106.29 \u00b1 1.73 104.87 \u00b1 0.39 hammer-human-v1 3.03 \u00b1 3.39 -0.19 \u00b1 0.02 1.02 \u00b1 0.24 1.01 \u00b1 0.51 0.14 \u00b1 0.11 1.79 \u00b1 0.80 0.24 \u00b1 0.24 0.24 \u00b1 0.00 0.28 \u00b1 0.18 1.28 \u00b1 0.15 hammer-cloned-v1 0.55 \u00b1 0.16 0.12 \u00b1 0.08 0.25 \u00b1 0.01 1.27 \u00b1 2.11 0.30 \u00b1 0.01 1.50 \u00b1 0.69 5.00 \u00b1 3.75 0.14 \u00b1 0.09 0.19 \u00b1 0.07 1.82 \u00b1 0.55 hammer-expert-v1 126.78 \u00b1 0.64 121.75 \u00b1 7.67 3.11 \u00b1 0.03 127.08 \u00b1 0.13 0.26 \u00b1 0.01 128.68 \u00b1 0.33 133.62 \u00b1 0.27 25.13 \u00b1 43.25 28.52 \u00b1 49.00 117.45 \u00b1 6.65 relocate-human-v1 0.04 \u00b1 0.03 -0.14 \u00b1 0.08 -0.29 \u00b1 0.01 0.45 \u00b1 0.53 0.06 \u00b1 0.03 0.12 \u00b1 0.04 0.16 \u00b1 0.30 -0.31 \u00b1 0.01 -0.17 \u00b1 0.17 0.05 \u00b1 0.01 relocate-cloned-v1 -0.06 \u00b1 0.01 -0.00 \u00b1 0.02 -0.30 \u00b1 0.01 -0.01 \u00b1 0.03 -0.29 \u00b1 0.01 0.04 \u00b1 0.01 1.66 \u00b1 2.59 -0.01 \u00b1 0.10 0.17 \u00b1 0.35 0.16 \u00b1 0.09 relocate-expert-v1 107.58 \u00b1 1.20 97.90 \u00b1 5.21 -1.73 \u00b1 0.96 109.52 \u00b1 0.47 -0.30 \u00b1 0.02 106.11 \u00b1 4.02 107.52 \u00b1 2.28 -0.36 \u00b1 0.00 71.94 \u00b1 18.37 104.28 \u00b1 0.42 adroit average 48.18 42.69 10.40 55.71 1.53 53.43 59.39 12.43 18.78 49.21"},{"location":"benchmarks/offline/#best-scores","title":"Best Scores","text":""},{"location":"benchmarks/offline/#gym-mujoco_1","title":"Gym-MuJoCo","text":"Task-Name BC 10% BC TD3+BC AWAC CQL IQL ReBRAC SAC-N EDAC DT halfcheetah-medium-v2 43.60 \u00b1 0.14 43.90 \u00b1 0.13 48.93 \u00b1 0.11 50.81 \u00b1 0.15 47.62 \u00b1 0.03 48.84 \u00b1 0.07 65.62 \u00b1 0.46 72.21 \u00b1 0.31 69.72 \u00b1 0.92 42.73 \u00b1 0.10 halfcheetah-medium-replay-v2 40.52 \u00b1 0.19 42.27 \u00b1 0.46 45.84 \u00b1 0.26 46.47 \u00b1 0.26 46.43 \u00b1 0.19 45.35 \u00b1 0.08 52.22 \u00b1 0.31 67.29 \u00b1 0.34 66.55 \u00b1 1.05 40.31 \u00b1 0.28 halfcheetah-medium-expert-v2 79.69 \u00b1 3.10 94.11 \u00b1 0.22 96.59 \u00b1 0.87 96.83 \u00b1 0.23 97.04 \u00b1 0.17 95.38 \u00b1 0.17 108.89 \u00b1 1.20 111.73 \u00b1 0.47 110.62 \u00b1 1.04 93.40 \u00b1 0.21 hopper-medium-v2 69.04 \u00b1 2.90 73.84 \u00b1 0.37 70.44 \u00b1 1.18 95.42 \u00b1 3.67 70.80 \u00b1 1.98 80.46 \u00b1 3.09 103.19 \u00b1 0.16 101.79 \u00b1 0.20 103.26 \u00b1 0.14 69.42 \u00b1 3.64 hopper-medium-replay-v2 68.88 \u00b1 10.33 90.57 \u00b1 2.07 98.12 \u00b1 1.16 101.47 \u00b1 0.23 101.63 \u00b1 0.55 102.69 \u00b1 0.96 102.57 \u00b1 0.45 103.83 \u00b1 0.53 103.28 \u00b1 0.49 88.74 \u00b1 3.02 hopper-medium-expert-v2 90.63 \u00b1 10.98 113.13 \u00b1 0.16 113.22 \u00b1 0.43 113.26 \u00b1 0.49 112.84 \u00b1 0.66 113.18 \u00b1 0.38 113.16 \u00b1 0.43 111.24 \u00b1 0.15 111.80 \u00b1 0.11 111.18 \u00b1 0.21 walker2d-medium-v2 80.64 \u00b1 0.91 82.05 \u00b1 0.93 86.91 \u00b1 0.28 85.86 \u00b1 3.76 84.77 \u00b1 0.20 87.58 \u00b1 0.48 87.79 \u00b1 0.19 90.17 \u00b1 0.54 95.78 \u00b1 1.07 74.70 \u00b1 0.56 walker2d-medium-replay-v2 48.41 \u00b1 7.61 76.09 \u00b1 0.40 91.17 \u00b1 0.72 86.70 \u00b1 0.94 89.39 \u00b1 0.88 89.94 \u00b1 0.93 91.11 \u00b1 0.63 85.18 \u00b1 1.63 89.69 \u00b1 1.39 68.22 \u00b1 1.20 walker2d-medium-expert-v2 109.95 \u00b1 0.62 109.90 \u00b1 0.09 112.21 \u00b1 0.06 113.40 \u00b1 2.22 111.63 \u00b1 0.38 113.06 \u00b1 0.53 112.49 \u00b1 0.18 116.93 \u00b1 0.42 116.52 \u00b1 0.75 108.71 \u00b1 0.34 locomotion average 70.15 80.65 84.83 87.80 84.68 86.28 93.00 95.60 96.36 77.49"},{"location":"benchmarks/offline/#maze2d_1","title":"Maze2d","text":"Task-Name BC 10% BC TD3+BC AWAC CQL IQL ReBRAC SAC-N EDAC DT maze2d-umaze-v1 16.09 \u00b1 0.87 22.49 \u00b1 1.52 99.33 \u00b1 16.16 136.96 \u00b1 10.89 92.05 \u00b1 13.66 50.92 \u00b1 4.23 162.28 \u00b1 1.79 153.12 \u00b1 6.49 149.88 \u00b1 1.97 63.83 \u00b1 17.35 maze2d-medium-v1 19.16 \u00b1 1.24 27.64 \u00b1 1.87 150.93 \u00b1 3.89 152.73 \u00b1 20.78 128.66 \u00b1 5.44 122.69 \u00b1 30.00 150.12 \u00b1 4.48 93.80 \u00b1 14.66 154.41 \u00b1 1.58 68.14 \u00b1 12.25 maze2d-large-v1 20.75 \u00b1 6.66 41.83 \u00b1 3.64 197.64 \u00b1 5.26 227.31 \u00b1 1.47 157.51 \u00b1 7.32 162.25 \u00b1 44.18 197.55 \u00b1 5.82 207.51 \u00b1 0.96 182.52 \u00b1 2.68 50.25 \u00b1 19.34 maze2d average 18.67 30.65 149.30 172.33 126.07 111.95 169.98 151.48 162.27 60.74"},{"location":"benchmarks/offline/#antmaze_1","title":"Antmaze","text":"Task-Name BC 10% BC TD3+BC AWAC CQL IQL ReBRAC SAC-N EDAC DT antmaze-umaze-v2 68.50 \u00b1 2.29 77.50 \u00b1 1.50 98.50 \u00b1 0.87 70.75 \u00b1 8.84 94.75 \u00b1 0.83 84.00 \u00b1 4.06 100.00 \u00b1 0.00 0.00 \u00b1 0.00 42.50 \u00b1 28.61 64.50 \u00b1 2.06 antmaze-umaze-diverse-v2 64.75 \u00b1 4.32 63.50 \u00b1 2.18 71.25 \u00b1 5.76 81.50 \u00b1 4.27 53.75 \u00b1 2.05 79.50 \u00b1 3.35 96.75 \u00b1 2.28 0.00 \u00b1 0.00 0.00 \u00b1 0.00 60.50 \u00b1 2.29 antmaze-medium-play-v2 4.50 \u00b1 1.12 6.25 \u00b1 2.38 3.75 \u00b1 1.30 25.00 \u00b1 10.70 80.50 \u00b1 3.35 78.50 \u00b1 3.84 93.50 \u00b1 2.60 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.75 \u00b1 0.43 antmaze-medium-diverse-v2 4.75 \u00b1 1.09 16.50 \u00b1 5.59 5.50 \u00b1 1.50 10.75 \u00b1 5.31 71.00 \u00b1 4.53 83.50 \u00b1 1.80 91.75 \u00b1 2.05 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.50 \u00b1 0.50 antmaze-large-play-v2 0.50 \u00b1 0.50 13.50 \u00b1 9.76 1.25 \u00b1 0.43 0.50 \u00b1 0.50 34.75 \u00b1 5.85 53.50 \u00b1 2.50 68.75 \u00b1 13.90 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 antmaze-large-diverse-v2 0.75 \u00b1 0.43 6.25 \u00b1 1.79 0.25 \u00b1 0.43 0.00 \u00b1 0.00 36.25 \u00b1 3.34 53.00 \u00b1 3.00 69.50 \u00b1 7.26 0.00 \u00b1 0.00 0.00 \u00b1 0.00 0.00 \u00b1 0.00 antmaze average 23.96 30.58 30.08 31.42 61.83 72.00 86.71 0.00 7.08 21.04"},{"location":"benchmarks/offline/#adroit_1","title":"Adroit","text":"Task-Name BC 10% BC TD3+BC AWAC CQL IQL ReBRAC SAC-N EDAC DT pen-human-v1 99.69 \u00b1 7.45 59.89 \u00b1 8.03 9.95 \u00b1 8.19 119.03 \u00b1 6.55 58.91 \u00b1 1.81 106.15 \u00b1 10.28 127.28 \u00b1 3.22 56.48 \u00b1 7.17 35.84 \u00b1 10.57 77.83 \u00b1 2.30 pen-cloned-v1 99.14 \u00b1 12.27 83.62 \u00b1 11.75 52.66 \u00b1 6.33 125.78 \u00b1 3.28 14.74 \u00b1 2.31 114.05 \u00b1 4.78 128.64 \u00b1 7.15 52.69 \u00b1 5.30 26.90 \u00b1 7.85 71.17 \u00b1 2.70 pen-expert-v1 128.77 \u00b1 5.88 134.36 \u00b1 3.16 142.83 \u00b1 7.72 162.53 \u00b1 0.30 14.86 \u00b1 4.07 140.01 \u00b1 6.36 157.62 \u00b1 0.26 116.43 \u00b1 40.26 36.04 \u00b1 4.60 119.49 \u00b1 2.31 door-human-v1 9.41 \u00b1 4.55 7.00 \u00b1 6.77 -0.11 \u00b1 0.06 17.70 \u00b1 2.55 13.28 \u00b1 2.77 13.52 \u00b1 1.22 0.27 \u00b1 0.43 -0.10 \u00b1 0.06 2.51 \u00b1 2.26 7.36 \u00b1 1.24 door-cloned-v1 3.40 \u00b1 0.95 10.37 \u00b1 4.09 -0.20 \u00b1 0.11 10.53 \u00b1 2.82 -0.08 \u00b1 0.13 9.02 \u00b1 1.47 7.73 \u00b1 6.80 -0.21 \u00b1 0.10 20.36 \u00b1 1.11 11.18 \u00b1 0.96 door-expert-v1 105.84 \u00b1 0.23 105.92 \u00b1 0.24 4.49 \u00b1 7.39 106.60 \u00b1 0.27 59.47 \u00b1 25.04 107.29 \u00b1 0.37 106.78 \u00b1 0.04 0.05 \u00b1 0.02 109.22 \u00b1 0.24 105.49 \u00b1 0.09 hammer-human-v1 12.61 \u00b1 4.87 6.23 \u00b1 4.79 2.38 \u00b1 0.14 16.95 \u00b1 3.61 0.30 \u00b1 0.05 6.86 \u00b1 2.38 1.18 \u00b1 0.15 0.25 \u00b1 0.00 3.49 \u00b1 2.17 1.68 \u00b1 0.11 hammer-cloned-v1 8.90 \u00b1 4.04 8.72 \u00b1 3.28 0.96 \u00b1 0.30 10.74 \u00b1 5.54 0.32 \u00b1 0.03 11.63 \u00b1 1.70 48.16 \u00b1 6.20 12.67 \u00b1 15.02 0.27 \u00b1 0.01 2.74 \u00b1 0.22 hammer-expert-v1 127.89 \u00b1 0.57 128.15 \u00b1 0.66 33.31 \u00b1 47.65 129.08 \u00b1 0.26 0.93 \u00b1 1.12 129.76 \u00b1 0.37 134.74 \u00b1 0.30 91.74 \u00b1 47.77 69.44 \u00b1 47.00 127.39 \u00b1 0.10 relocate-human-v1 0.59 \u00b1 0.27 0.16 \u00b1 0.14 -0.29 \u00b1 0.01 1.77 \u00b1 0.84 1.03 \u00b1 0.20 1.22 \u00b1 0.28 3.70 \u00b1 2.34 -0.18 \u00b1 0.14 0.05 \u00b1 0.02 0.08 \u00b1 0.02 relocate-cloned-v1 0.45 \u00b1 0.31 0.74 \u00b1 0.45 -0.02 \u00b1 0.04 0.39 \u00b1 0.13 -0.07 \u00b1 0.02 1.78 \u00b1 0.70 9.25 \u00b1 2.56 0.10 \u00b1 0.04 4.11 \u00b1 1.39 0.34 \u00b1 0.09 relocate-expert-v1 110.31 \u00b1 0.36 109.77 \u00b1 0.60 0.23 \u00b1 0.27 111.21 \u00b1 0.32 0.03 \u00b1 0.10 110.12 \u00b1 0.82 111.14 \u00b1 0.23 -0.07 \u00b1 0.08 98.32 \u00b1 3.75 106.49 \u00b1 0.30 adroit average 58.92 54.58 20.51 67.69 13.65 62.62 69.71 27.49 33.88 52.60"},{"location":"benchmarks/offline/#visual-summary","title":"Visual summary","text":""},{"location":"benchmarks/repro/","title":"How to Reproduce","text":"<p>To reproduce all figures and tables from our technical paper, do the following steps.</p>"},{"location":"benchmarks/repro/#collect-wandb-logs","title":"Collect wandb logs","text":"<p>These scripts collect all wandb logs into .csv files and save them into the <code>runs_tables</code> folder.  We provide the tables, but you can recollect them. <pre><code>python results/get_offline_urls.py\npython results/get_finetune_urls.py\n</code></pre></p>"},{"location":"benchmarks/repro/#collect-scores","title":"Collect scores","text":"<p>These scripts collect data from runs kept in .csv files and save evaluation scores (and regret in case of offline-to-online)  into pickled files, which are stored in the <code>bin</code> folder. We provide the pickled data, but if you need to extract more data, you can modify scripts for your purposes. <pre><code>python results/get_offline_scores.py\npython results/get_finetune_scores.py\n</code></pre></p>"},{"location":"benchmarks/repro/#print-tables","title":"Print tables","text":"<p>These scripts use pickled data, print all the tables, and save all figures into the <code>out</code> directory. <pre><code>python results/get_offline_tables_and_plots.py\npython results/get_finetune_tables_and_plots.py\n</code></pre></p>"},{"location":"community/contrib/","title":"Contribution","text":""},{"location":"community/contrib/#contributing-to-the-codebase","title":"Contributing to the codebase","text":"<p>We welcome:</p> <ul> <li>Bug reports</li> <li>Pull requests for bug fixes</li> <li>Logs and documentation improvements</li> <li>New algorithms and datasets</li> <li>Better hyperparameters (but with proofs)</li> </ul>"},{"location":"community/contrib/#setup","title":"Setup","text":"<p>Contributing code is done through standard github methods:</p> <ol> <li>Fork this repo</li> <li>Make a change and commit your code</li> <li>Submit a pull request. It will be reviewed by maintainers, and they'll give feedback or make requests as applicable</li> </ol> <pre><code>git clone git@github.com:tinkoff-ai/CORL.git\ncd CORL\npip install -r requirements/requirements_dev.txt\n</code></pre> <p>For dependencies installation see get started section.</p>"},{"location":"community/contrib/#code-style","title":"Code style","text":"<p>The CI will run several checks on the new code pushed to the CORL repository.  These checks can also be run locally without waiting for the CI by following the steps below:</p> <ol> <li>install <code>pre-commit</code>,</li> <li>install the Git hooks by running <code>pre-commit install</code>.</li> </ol> <p>Once those two steps are done, the Git hooks will be run automatically at every new commit.  The Git hooks can also be run manually with <code>pre-commit run --all-files</code>, and if needed they can be skipped (not recommended) with <code>git commit --no-verify</code>.</p> <p>We use Ruff as our main linter. If you want to see possible  problems before pre-commit, you can run <code>ruff check --diff .</code> to see exact linter suggestions and future fixes.</p>"},{"location":"community/contrib/#adding-new-algorithms","title":"Adding new algorithms","text":"<p>Warning</p> <pre><code>While we welcome any algorithms, it is better to open an issue with the proposal before \nso we can discuss the details. Unfortunately, not all algorithms are equally \neasy to understand and reproduce. We may be able to give a couple of advices to you,\nor on the contrary warn you that this particular algorithm will require too much \ncomputational resources to fully reproduce the results, and it is better to do something else.\n</code></pre> <p>All new algorithms should go to the <code>algorithms/contrib/offline</code> for just  offline algorithms and to the <code>algorithms/contrib/finetune</code> for the offline-to-online algorithms. </p> <p>We as a team try to keep the core as reliable and reproducible as possible,  but we may not have the resources to support all future algorithms.  Therefore, this separation is necessary, as we cannot guarantee that all  algorithms from <code>algorithms/contrib</code> exactly reproduce the results of their original publications.</p> <p>Make sure your new code is properly documented and all references to the original implementations and papers are present (for example as in Decision Transformer).  Follow the conventions for naming argument of configs, functions, classes. Try to stylistically imitate already existing implementations.</p> <p>Please, explain all the tricks and possible differences from the original implementation in as much detail as possible.  Keep in mind that this code may be used by other researchers. Make their lives easier!</p>"},{"location":"community/contrib/#running-benchmarks","title":"Running benchmarks","text":"<p>Although you will have to do a hyperparameter search while reproducing the algorithm,  in the end we expect to see final configs in <code>configs/contrib/&lt;algo_type&gt;/&lt;algo_name&gt;/&lt;dataset_name&gt;.yaml</code> with the best hyperparameters for all  datasets considered. The configs should be in <code>yaml</code> format, containing all hyperparameters sorted  in alphabetical order (see existing configs for an inspiration).</p> <p>Use these conventions to name your runs in the configs: 1. <code>name: &lt;algo_name&gt;</code> 2. <code>group: &lt;algo_name&gt;-&lt;dataset_name&gt;-multiseed-v0</code>, increment version if needed 3. use our __post_init__ implementation in your config dataclass</p> <p>Since we are releasing wandb logs for all algorithms, you will need to submit multiseed (~4 seeds)  training runs the <code>CORL</code> project in the wandb corl-team organization. We'll invite you there when the time will come.</p> <p>We usually use wandb sweeps for this. You can use this example config (it will work with pyrallis as it expects <code>config_path</code> cli argument): sweep_config.yaml<pre><code>entity: corl-team\nproject: CORL\nprogram: algorithms/contrib/&lt;algo_name&gt;.py\nmethod: grid\nparameters:\n  config_path:\n    # algo_type is offline or finetune (see sections above)\n    values: [\n        \"configs/contrib/&lt;algo_type&gt;/&lt;algo_name&gt;/&lt;dataset_name_1&gt;.yaml\",\n        \"configs/contrib/&lt;algo_type&gt;/&lt;algo_name&gt;/&lt;dataset_name_2&gt;.yaml\",\n        \"configs/contrib/&lt;algo_type&gt;/&lt;algo_name&gt;/&lt;dataset_name_3&gt;.yaml\",\n    ]\n  train_seed:\n    values: [0, 1, 2, 3]\n</code></pre> Then proceed as usual. Create wandb sweep with <code>wandb sweep sweep_config.yaml</code>, then run agents with <code>wandb agent &lt;agent_id&gt;</code>.</p> <p>Based on the results, you will need to make wandb reports to make it easier for other users to understand.  You can use any of the already existing ones as an example (see README.md).</p>"},{"location":"community/contrib/#checklist","title":"Checklist","text":"<p>Ideally, all checks should be completed!</p> <ul> <li> Issue about new algorithm is open</li> <li> Single-file implementation is added to the <code>algorithms/contrib</code></li> <li> PR has passed all the tests</li> <li> Evidence that implementation reproduces original results is provided</li> <li> Configs with the best hyperparameters for all datasets are added to the <code>configs/contrib</code></li> <li> Logs and reports for best hyperparameters are submitted to our wandb organization</li> </ul>"},{"location":"community/publications/","title":"List of Publications","text":"<p>Tip</p> <pre><code>Please open a pull request to add missing entries!\n</code></pre> <p>List of publications that are using CORL algorithms or benchmarked results:</p> <ul> <li>Lu, C., Ball, P. J., &amp; Parker-Holder, J. Synthetic Experience Replay.</li> <li>Beeson, A., &amp; Montana, G. (2023). Balancing policy constraint and ensemble size in uncertainty-based offline reinforcement learning. arXiv preprint arXiv:2303.14716.</li> <li>Nikulin, A., Kurenkov, V., Tarasov, D., &amp; Kolesnikov, S. (2023). Anti-exploration by random network distillation. arXiv preprint arXiv:2301.13616.</li> <li>Bhargava, P., Chitnis, R., Geramifard, A., Sodhani, S., &amp; Zhang, A. (2023). Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. arXiv preprint arXiv:2305.14550.</li> <li>Hu, X., Ma, Y., Xiao, C., Zheng, Y., &amp; Meng, Z. (2023). In-Sample Policy Iteration for Offline Reinforcement Learning. arXiv preprint arXiv:2306.05726.</li> <li>Lian, S., Ma, Y., Liu, J., Zheng, Y., &amp; Meng, Z. (2023). HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach. arXiv preprint arXiv:2306.06329.</li> <li>He, H., Bai, C., Xu, K., Yang, Z., Zhang, W., Wang, D., ... &amp; Li, X. (2023). Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. arXiv preprint arXiv:2305.18459.</li> <li>Liu, J., Ma, Y., Hao, J., Hu, Y., Zheng, Y., Lv, T., &amp; Fan, C. (2023). Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning. arXiv preprint arXiv:2306.15503.</li> <li>Chitnis, R., Xu, Y., Hashemi, B., Lehnert, L., Dogan, U., Zhu, Z., &amp; Delalleau, O. (2023). IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control. arXiv preprint arXiv:2306.00867.</li> <li>Kurenkov, V., Nikulin, A., Tarasov, D., &amp; Kolesnikov, S. (2023). Katakomba: Tools and Benchmarks for Data-Driven NetHack. arXiv preprint arXiv:2306.08772.</li> <li>Lian, S., Ma, Y., Liu, J., Jianye, H. A. O., Zheng, Y., &amp; Meng, Z. (2023, July). A Policy-Decoupled Method for High-Quality Data Augmentation in Offline Reinforcement Learning. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems.</li> </ul>"},{"location":"get-started/install/","title":"Installation","text":""},{"location":"get-started/install/#manual","title":"Manual","text":"<p>Warning</p> <pre><code>Unfortunately, installing all dependencies can cause some difficulties at the moment, mainly due to **D4RL** and \nthe old version of mujoco it is locked to. It will be much easier in the future after migration to the **Minari** is done.\n</code></pre> <p>All necessary dependencies are specified in the <code>requirements/requirements.txt</code> file.  You can just clone the repo and install all dependencies with pip:  <pre><code>git clone https://github.com/corl-team/CORL.git\ncd CORL\npip install -r requirements/requirements.txt\n</code></pre></p> <p>In addition to those specified there, the dependencies required by D4RL, namely MuJoCo binaries, must also be installed. We recommend following the official guide from mujoco-py. You will need to download MuJoCo 2.1 binaries and extract downloaded <code>mujoco210</code> directory to the <code>~/.mujoco/mujoco210</code>: <pre><code>mkdir -p ~/.mujoco \\\n    &amp;&amp; wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz \\\n    &amp;&amp; tar -xf mujoco.tar.gz -C ~/.mujoco \\\n    &amp;&amp; rm mujoco.tar.gz\nexport LD_LIBRARY_PATH=~/.mujoco/mujoco210/bin:${LD_LIBRARY_PATH}\n</code></pre> If you have any problems with the installation, we advise you to first look for similar issues in the  original D4RL and mujoco-py repositories. Most likely problem is in D4RL, not in CORL </p>"},{"location":"get-started/install/#docker","title":"Docker","text":"<p>To simplify installation and improve reproducibility, we provide a preconfigured Dockerfile that you can use: <pre><code>cd CORL\ndocker build -t corl .\ndocker run --gpus=all -it --rm --name corl-container corl\n</code></pre></p>"},{"location":"get-started/usage/","title":"Basic Usage","text":""},{"location":"get-started/usage/#how-to-train","title":"How to Train","text":"<p>We use pyrallis for the configuration, thus after the dependencies have been installed,  there are two ways to run the CORL algorithms:</p> <ol> <li> <p>Manually specifying all the arguments within the terminal (they will overwrite the default ones): <pre><code>python algorithms/offline/dt.py \\\n    --project=\"CORL-Test\" \\\n    --group=\"DT-Test\" \\\n    --name=\"dt-testing-run\" \\\n    --env_name=\"halfcheetah-medium-v2\" \\\n    --device=\"cuda:0\"\n    # etc...\n</code></pre></p> </li> <li> <p>With yaml config. First, create yaml file with all needed hyperparameters: dt_example_config.yaml<pre><code># taken from https://github.com/corl-team/CORL/blob/main/configs/offline/dt/halfcheetah/medium_v2.yaml\nattention_dropout: 0.1\nbatch_size: 4096\nbetas:\n- 0.9\n- 0.999\ncheckpoints_path: null\nclip_grad: 0.25\ndeterministic_torch: false\ndevice: cuda\nembedding_dim: 128\nembedding_dropout: 0.1\nenv_name: \"halfcheetah-medium-v2\"\nepisode_len: 1000\neval_episodes: 100\neval_every: 5000\neval_seed: 42\ngroup: \"dt-halfcheetah-medium-v2-multiseed-v2\"\nlearning_rate: 0.0008\nmax_action: 1.0\nname: \"DT\"\nnum_heads: 1\nnum_layers: 3\nnum_workers: 4\nproject: \"CORL\"\nresidual_dropout: 0.1\nreward_scale: 0.001\nseq_len: 20\ntarget_returns: [12000.0, 6000.0]\ntrain_seed: 10\nupdate_steps: 100000\nwarmup_steps: 10000\nweight_decay: 0.0001\n</code></pre> After that we can supply all hyperparameters from config with <code>config_path</code> argument: <pre><code>python algorithms/offline/dt.py \\\n    --config_path=\"dt_example_config.yaml\"\n    # you can also overwrite any hyperparameter if needed\n    --device=\"cuda:0\"\n    # etc...\n</code></pre> By default, training script will log metrics to the wandb project specified by the <code>group</code> argument.  If you want to disable logging, run <code>wandb disabled</code> or <code>wandb offline</code>. To turn it back on, run <code>wandb online</code>.  For more options see wandb documentation.    </p> <p>If you're not familiar with Weights &amp; Biases logging tools, it is better to first familiarize  yourself with the basics here. </p> <p>For an explanation of all logged metrics, refer to the documentation of the specific algorithm.</p> </li> </ol>"},{"location":"get-started/usage/#cli-documentation","title":"CLI Documentation","text":"<p>How to find out all available hyperparameters and their brief explanation? Very simple, just run <code>python algorithms/offline/dt.py --help</code> (this will work for all algorithms): <pre><code>usage: dt.py [-h] [--config_path str] [--project str] [--group str] [--name str] [--embedding_dim int] [--num_layers int]\n             [--num_heads int] [--seq_len int] [--episode_len int] [--attention_dropout float] [--residual_dropout float]\n             [--embedding_dropout float] [--max_action float] [--env_name str] [--learning_rate float]\n             [--betas float float] [--weight_decay float] [--clip_grad [float]] [--batch_size int] [--update_steps int]\n             [--warmup_steps int] [--reward_scale float] [--num_workers int] [--target_returns float [float, ...]]\n             [--eval_episodes int] [--eval_every int] [--checkpoints_path [str]] [--deterministic_torch bool]\n             [--train_seed int] [--eval_seed int] [--device str]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --config_path str     Path for a config file to parse with pyrallis (default: None)\n\nTrainConfig:\n\n  --project str         wandb project name (default: CORL)\n  --group str           wandb group name (default: DT-D4RL)\n  --name str            wandb run name (default: DT)\n  --embedding_dim int   transformer hidden dim (default: 128)\n  --num_layers int      depth of the transformer model (default: 3)\n  --num_heads int       number of heads in the attention (default: 1)\n  --seq_len int         maximum sequence length during training (default: 20)\n  --episode_len int     maximum rollout length, needed for the positional embeddings (default: 1000)\n  --attention_dropout float\n                        attention dropout (default: 0.1)\n  --residual_dropout float\n                        residual dropout (default: 0.1)\n  --embedding_dropout float\n                        embeddings dropout (default: 0.1)\n  --max_action float    maximum range for the symmetric actions, [-1, 1] (default: 1.0)\n  --env_name str        training dataset and evaluation environment (default: halfcheetah-medium-v2)\n  --learning_rate float\n                        AdamW optimizer learning rate (default: 0.0001)\n  --betas float float   AdamW optimizer betas (default: (0.9, 0.999))\n  --weight_decay float  AdamW weight decay (default: 0.0001)\n  --clip_grad [float]   maximum gradient norm during training, optional (default: 0.25)\n  --batch_size int      training batch size (default: 64)\n  --update_steps int    total training steps (default: 100000)\n  --warmup_steps int    warmup steps for the learning rate scheduler (increasing from zero to learning_rate) (default:\n                        10000)\n  --reward_scale float  reward scaling, to reduce the magnitude (default: 0.001)\n  --num_workers int     number of workers for the pytorch dataloader (default: 4)\n  --target_returns float [float, ...]\n                        target return-to-go for the prompting durint evaluation (default: (12000.0, 6000.0))\n  --eval_episodes int   number of episodes to run during evaluation (default: 100)\n  --eval_every int      evaluation frequency, will evaluate eval_every training steps (default: 10000)\n  --checkpoints_path [str]\n                        path for checkpoints saving, optional (default: None)\n  --deterministic_torch bool\n                        configure PyTorch to use deterministic algorithms instead of nondeterministic ones where available\n                        (default: False)\n  --train_seed int      training random seed (default: 10)\n  --eval_seed int       evaluation random seed (default: 42)\n  --device str          training device (default: cuda)\n</code></pre></p>"},{"location":"get-started/usage/#benchmarking","title":"Benchmarking","text":"<p>Sooner or later you will probably want to run many experiments at once, for example to search for hyperparameters,  or to do multi-seed training for some datasets. For something like this we recommend using wandb sweeps (and we use them ourselves).  The general recipe looks like this. First, create wandb seep config: sweep_config.yaml<pre><code>entity: corl-team\nproject: CORL\nprogram: algorithms/offline/dt.py\nmethod: grid\nparameters:\n  # specify all configs to run for the choosen algorithm\n  config_path:\n    values: [\n        \"configs/offline/dt/halfcheetah/medium_v2.yaml\",\n        \"configs/offline/dt/halfcheetah/medium_replay_v2.yaml\",\n        \"configs/offline/dt/halfcheetah/medium_expert_v2.yaml\",\n    ]\n  train_seed:\n    values: [0, 1, 2, 3]\n</code></pre> Then proceed as usual. Create wandb sweep with <code>wandb sweep sweep_config.yaml</code>, then run agents with <code>wandb agent &lt;agent_id&gt;</code>.  This will train multiple seeds for each config.</p> <p>All configs with full hyperparameters for all datasets and algorithms are in <code>configs</code>.</p>"}]}